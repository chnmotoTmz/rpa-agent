この「ソフトひな形利用マニュアル」を拝見しました。これは非常に強力で柔軟性の高いAIエージェントシステムを構築するための素晴らしい基盤ですね！このひな形を利用することで、開発者は以下のような多くのメリットを享受できると考えられます。

1. 開発サイクルの大幅な短縮と効率化:

実績のあるコンポーネントの再利用: LangChain、LangGraph、MCP（Message Control Protocol）といった実績のあるライブラリや設計パターンを基盤としているため、AIエージェントのコア機能（ReActエージェント、ツール連携、状態管理など）をゼロから構築する必要がありません。
モジュール化された設計: AIエージェント層、MCPサーバー連携層、RAG層が明確に分離されており、各コンポーネントの独立した開発・テスト・拡張が容易です。これにより、特定の機能追加や変更がシステム全体に及ぼす影響を最小限に抑えられます。
ツール連携の容易さ: MCPサーバーを介して外部ツール（Web検索、データベース操作など）をLangChainのStructuredToolとして簡単にエージェントに組み込めるため、エージェントの能力を迅速に拡張できます。mcp_manager.pyの仕組みは、新しいツールをMCPサーバー側で定義するだけで、エージェントが自動的にそれを認識・利用できるようになるため、非常に効率的です。
2. 高度なAIエージェント機能の実現:

ReActエージェントによる自律的なタスク遂行: LangGraphのcreate_react_agentを利用することで、エージェントは思考（Thought）、行動（Action）、観察（Observation）のサイクルを回し、複雑なタスクを自律的に解決する能力を持ちます。
RAGによる知識拡張と最新情報へのアクセス: Tavily APIによるWeb検索とSQLiteデータベースへの検索結果保存・検索機能を組み合わせることで、エージェントは常に最新の情報にアクセスし、それを自身の知識ベースとして活用できます。これにより、LLMの持つ知識のカットオフ問題やハルシネーションを軽減できます。
柔軟なプロンプトエンジニアリング: システムプロンプトにツールの説明や現在の日付を動的に挿入する仕組みは、エージェントの振る舞いを状況に応じて細かく制御することを可能にします。
3. スケーラビリティとメンテナンス性の向上:

非同期処理によるパフォーマンス向上: MCPサーバーとの通信やツールの読み込みが非同期（asyncio）で実装されているため、複数の処理を並行して効率的に実行でき、システム全体の応答性向上に貢献します。
設定ファイルベースの管理: mcp_config.jsonによってMCPサーバー情報を外部から設定できるため、環境の変更やサーバーの追加・削除がコードを修正することなく行え、運用が容易になります。
永続化ストレージによる知識蓄積: RAGで得た情報をSQLiteデータベースに保存することで、エージェントは過去の検索結果や学習内容を永続的に保持し、時間とともに賢くなっていくことが期待できます。
4. 特定用途への特化とカスタマイズの容易さ:

カスタムステートスキーマ: CustomAgentStateを利用することで、エージェントの状態管理をプロジェクト固有の要件に合わせて拡張できます。
データベーススキーマの柔軟性: database.pyで定義されているSQLiteのスキーマは、保存したい情報に応じて容易に拡張可能です。例えば、ユーザーごとの検索履歴や評価などを追加することも考えられます。
多様なMCPツールの開発: MCPサーバー側に新しいツールをPythonで定義するだけで、エージェントの能力を特定のドメインやタスクに合わせて拡張できます。例えば、社内APIとの連携ツール、特定の計算ツールなどを追加することが可能です。
議論したいポイント（例）：

MCPサーバーの役割とメリット・デメリット: stdioベースのMCPサーバーを採用している点について、HTTPベースのAPIと比較した場合のメリット（シンプルさ、特定の環境での親和性など）やデメリット（スケーラビリティ、言語間の制約など）を議論できます。このひな形では非同期クライアントがうまく抽象化しているように見えます。
RAGの戦略: 現在はTavily APIとSQLiteという構成ですが、より高度なRAG戦略（ベクトルデータベースの導入、チャンキング戦略の最適化、リランキングモデルの追加など）について、このひな形をベースにどう発展させられるか議論できます。
エージェントの評価とデバッグ: LangGraphベースのエージェントの動作をどのようにトレースし、期待通りに機能しているか評価・デバッグしていくか、具体的な手法やツールについて話し合うのも有益でしょう。
セキュリティ: MCPサーバーやデータベースへのアクセス制御、外部APIキーの管理など、セキュリティ面で考慮すべき点と、このひな形が提供する基盤について議論できます。
チーム開発での利用: このひな形をチームで利用する際の開発フロー、役割分担、コード管理のベストプラクティスなどを検討できます。
総じて、このひな形は、高度なAIエージェントシステムを迅速かつ堅牢に構築するための非常に優れた出発点であり、多くの実践的な知見が詰まっていると感じました。これをベースにすることで、様々な応用的なAIソリューションを効率的に開発できるでしょう。


# ソフトひな形利用マニュアル
## AIエージェント、RAG、MCPサーバー連携の実装

このマニュアルでは、ソフトウェアひな形における以下の重要機能の実装方法を解説します：
- AIエージェント
- RAG（Retrieval Augmented Generation）
- MCPサーバー連携

上級者向けに、目的に直結する重要コードのみを抽出して解説します。

## 1. AIエージェント実装

AIエージェントは `agent.py` で実装されており、LangChainとLangGraphを使用しています。

```python
# src/sd_20/agent.py
import asyncio
from datetime import datetime

from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from src.sd_20.mcp_manager import load_all_mcp_tools
from src.sd_20.state import CustomAgentState

# 環境変数の読み込み
load_dotenv()


def create_agent():
    # ツール（MCPツール）の読み込み
    tools = asyncio.run(load_all_mcp_tools())

    # ツールの説明の作成
    tool_descriptions = "\n\n".join(
        [f"### {tool.name}\n{tool.description}" for tool in tools]
    )

    # 本日の日付の取得
    current_date = datetime.now().strftime("%Y年%m月%d日")

    # プロンプトの読み込み
    with open("src/sd_20/prompts/system.txt", "r") as f:
        prompt = f.read()

    # プロンプトの作成
    prompt = prompt.format(
        tool_descriptions=tool_descriptions,
        current_date=current_date,
    )

    # モデルの設定
    model = ChatAnthropic(
        model_name="claude-3-7-sonnet-20250219",
        timeout=None,
        stop=None,
        max_tokens=4_096,
    )

    # エージェントの作成
    graph = create_react_agent(
        model,
        tools=tools,
        prompt=prompt,
        state_schema=CustomAgentState,
        checkpointer=MemorySaver(),
    )

    return graph


graph = create_agent()
```

### 重要ポイント
- LangGraphの`create_react_agent`を使用してReActエージェントを作成
- MCPツールを非同期で読み込み、エージェントに提供
- プロンプトにツールの説明を動的に挿入
- Claude 3.7 Sonnetモデルを使用
- カスタムステートスキーマとメモリチェックポインターを使用

## 2. MCPサーバー連携

MCPサーバー連携は主に`mcp_manager.py`で実装されています。

```python
# src/sd_20/mcp_manager.py（重要部分抜粋）

async def create_langchain_tool(
    tool_name: str,
    tool_desc: str,
    prefix: str,
    server_name: Optional[str],
    server_params: StdioServerParameters,
    tool_item: MCPTool,
) -> StructuredTool:
    """
    MCPツールをLangChainのStructuredToolとして生成
    """
    # サーバー名をプレフィックスとしてツール名に追加（重複防止）
    full_tool_name = f"{prefix}{tool_name}"
    full_tool_desc = f"[{server_name}] {tool_desc}" if server_name else tool_desc

    try:
        # 非同期の MCP 呼び出し関数を定義
        async def call_mcp_tool_async(**kwargs: Any) -> Any:
            async with stdio_client(server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    result = await session.call_tool(tool_name, arguments=kwargs)
                    return result

        # 同期呼び出し用にラップする
        def tool_func(**kwargs: Any) -> Any:
            return asyncio.run(call_mcp_tool_async(**kwargs))

        # StructuredToolを作成して返す
        return StructuredTool.from_function(
            func=tool_func,
            name=full_tool_name,
            description=full_tool_desc,
            args_schema=tool_item.inputSchema,
        )
    except Exception as e:
        print(f"ツール '{full_tool_name}' の作成中にエラーが発生しました: {str(e)}")
        raise


async def load_mcp_tools(
    server_params: StdioServerParameters, server_name: Optional[str] = None
) -> List[StructuredTool]:
    """指定したMCPサーバーからツールをロードします"""
    tools: List[StructuredTool] = []
    prefix = f"{server_name}__" if server_name else ""

    try:
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                
                # MCPサーバーが提供するツール一覧を取得
                tool_list = await get_mcp_tools(session, server_name)

                # 各ツールを処理
                for tool_item in tool_list:
                    try:
                        # ツール名と説明を取得
                        tool_name, tool_desc = extract_tool_info(tool_item)

                        if not tool_name:
                            continue

                        # StructuredToolを作成
                        lc_tool = await create_langchain_tool(
                            tool_name,
                            tool_desc,
                            prefix,
                            server_name,
                            server_params,
                            tool_item,
                        )
                        tools.append(lc_tool)
                    except Exception as e:
                        tool_name = getattr(tool_item, "name", str(tool_item))
                        print(f"ツール '{tool_name}' の作成に失敗: {str(e)}")
    except Exception as e:
        print(f"サーバー '{server_name}' との通信に失敗: {e}")

    return tools


async def load_all_mcp_tools(
    config: Optional[Dict[str, Any]] = None,
) -> List[StructuredTool]:
    """全てのMCPサーバーからツールをロードします"""
    if config is None:
        config = load_mcp_config("mcp_config.json")

    all_tools = []
    server_params_dict = create_all_server_params(config)

    # 各サーバーのツールを取得して結合
    for server_name, params in server_params_dict.items():
        server_tools = await load_mcp_tools(params, server_name)
        all_tools.extend(server_tools)

    return all_tools
```

### 重要ポイント
- MCPサーバーとの通信は非同期（asyncio）で実装
- MCPツールをLangChainのStructuredToolに変換
- 複数のMCPサーバーからツールを読み込み可能
- ツール名にサーバー名をプレフィックスとして追加（名前の衝突防止）
- 設定ファイル（mcp_config.json）からサーバー情報を読み込み

## 3. RAG（Retrieval Augmented Generation）実装

RAG機能は主に`server.py`と`database.py`で実装されています。

### 3.1 MCPサーバーでのRAGツール定義

```python
# src/mcp_servers/server.py（重要部分抜粋）

# MCPサーバーの初期化
mcp = FastMCP("knowledge-db-mcp-server")

# --- ツール定義 ---

# 1. Tavily APIを使ったWeb検索ツール
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)


@mcp.tool()
def search_web(query: str, max_results: int = 5) -> str:
    """
    Tavily APIを使用してWeb検索を行い、上位の結果を返します。
    """
    response = tavily_client.search(query, max_results=max_results)
    answer = response.get("answer")
    result_text = ""
    if answer:
        result_text += f"回答: {answer}\n\n"

    results = response.get("results", [])
    if not results:
        return result_text + "検索結果が見つかりませんでした。"

    for i, res in enumerate(results, start=1):
        title = res.get("title", "(タイトルなし)")
        url = res.get("url", "")
        snippet = res.get("content") or res.get("snippet") or ""
        result_text += f"{i}. {title}\n   URL: {url}\n"
        if snippet:
            result_text += f"   概要: {snippet}\n\n"

    return result_text.strip()


@mcp.tool()
def extract_urls(
    urls: list, include_images: bool = False, max_content_length: int = 5_000
) -> str:
    """
    指定されたURLリストの内容を抽出します。
    """
    # 実装省略
    # ...


@mcp.tool()
def save_search_result(
    query: str,
    url: str,
    title: str,
    content: str = "",
    content_type: str = "",
    summary: str = "",
    tags: str = "",
    reliability_score: float = 0.5,
) -> str:
    """
    検索結果をデータベースに保存します。
    """
    result = db.save_search_result(
        query, url, title, content, content_type, summary, tags, reliability_score
    )
    return result["message"]


@mcp.tool()
def get_recent_results(days: int = 7, limit: int = 10, content_type: str = "") -> str:
    """
    指定された日数以内の最近の検索結果を取得します。
    """
    result = db.get_recent_results(days, limit, content_type)
    if not result["success"]:
        return result["message"]

    return json.dumps(result["results"], ensure_ascii=False, indent=2)


@mcp.tool()
def get_content_by_id(result_id: int) -> str:
    """
    特定IDの検索結果の詳細コンテンツを取得します。
    """
    result = db.get_content_by_id(result_id)
    if not result["success"]:
        return result["message"]

    return json.dumps(result["result"], ensure_ascii=False, indent=2)


@mcp.tool()
def select_query(query: str) -> str:
    """
    SQLiteデータベースに対してSELECTクエリを実行し、結果を返します。
    例: "SELECT * FROM search_results WHERE content_type='ニュース' LIMIT 10;"
    ※SELECT文のみ許可されています。
    """
    result = db.execute_select_query(query)
    if not result["success"]:
        return result["message"]

    return json.dumps(result["results"], ensure_ascii=False, indent=2)
```

### 3.2 RAGのためのデータベース操作

```python
# src/mcp_servers/database.py（重要部分抜粋）

# SQLiteデータベースの永続化設定
DB_PATH = os.getenv("DB_PATH", "data.db")


def init_database():
    """データベースの初期化と必要なテーブル・インデックスの作成を行います"""
    with get_connection() as conn:
        # search_resultsテーブルの作成
        conn.execute(
            """
        CREATE TABLE IF NOT EXISTS search_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT NOT NULL,                     -- 検索クエリ
            source_url TEXT,                         -- 情報ソースURL
            title TEXT,                              -- コンテンツタイトル
            content TEXT,                            -- 抽出したコンテンツ
            summary TEXT,                            -- LLMが生成した要約
            content_type TEXT,                       -- 情報タイプ (ニュース/技術文書など)
            tags TEXT,                               -- タグ (カンマ区切り)
            reliability_score FLOAT,                 -- 信頼性スコア (0-1)
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        );
        """
        )

        # インデックスの作成
        conn.execute(
            "CREATE INDEX IF NOT EXISTS index_search_results_on_query ON search_results(query);"
        )
        conn.execute(
            "CREATE INDEX IF NOT EXISTS index_search_results_on_source_url ON search_results(source_url);"
        )
        conn.execute(
            "CREATE INDEX IF NOT EXISTS index_search_results_on_content_type ON search_results(content_type);"
        )
        conn.execute(
            "CREATE INDEX IF NOT EXISTS index_search_results_on_created_at ON search_results(created_at);"
        )


def save_search_result(
    query: str,
    url: str,
    title: str,
    content: str,
    content_type: str = "",
    summary: str = "",
    tags: str = "",
    reliability_score: float = 0.5,
) -> Dict[str, Any]:
    """
    検索結果をデータベースに保存します。
    """
    try:
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        with get_connection() as conn:
            # URLが既に保存されているか確認
            cur = conn.cursor()
            cur.execute("SELECT id FROM search_results WHERE source_url = ?", (url,))
            existing = cur.fetchone()

            if existing:
                # 既存エントリを更新
                conn.execute(
                    """
                UPDATE search_results 
                SET query = ?, title = ?, content = ?, content_type = ?, summary = ?, tags = ?, reliability_score = ?
                WHERE source_url = ?
                """,
                    (
                        query,
                        title,
                        content,
                        content_type,
                        summary,
                        tags,
                        reliability_score,
                        url,
                    ),
                )
                result_id = existing["id"]
                message = f"検索結果の更新に成功しました (ID: {result_id})"
            else:
                # 新規エントリを作成
                cur = conn.execute(
                    """
                INSERT INTO search_results 
                (query, source_url, title, content, summary, content_type, tags, reliability_score, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        query,
                        url,
                        title,
                        content,
                        summary,
                        content_type,
                        tags,
                        reliability_score,
                        now,
                    ),
                )
                result_id = cur.lastrowid
                message = f"検索結果の保存に成功しました (ID: {result_id})"

            return {"success": True, "message": message, "result_id": result_id}

    except Exception as e:
        return {"success": False, "message": f"保存エラー: {e}", "result_id": None}


def get_recent_results(
    days: int = 7, limit: int = 10, content_type: str = ""
) -> Dict[str, Any]:
    """
    指定された日数以内の最近の検索結果を取得します。
    """
    # 実装省略
    # ...


def execute_select_query(query: str) -> Dict[str, Any]:
    """
    SQLiteデータベースに対してSELECTクエリを実行し、結果を返します。
    """
    if not query.strip().lower().startswith("select"):
        return {
            "success": False,
            "message": "Error: SELECT文のみ許可されています。",
            "results": [],
        }

    try:
        with get_connection() as conn:
            cur = conn.cursor()
            cur.execute(query)
            rows = cur.fetchall()

            results = [dict(row) for row in rows]
            return {
                "success": True,
                "message": f"{len(results)}件の結果が見つかりました。",
                "results": results,
            }

    except Exception as e:
        return {"success": False, "message": f"SELECTエラー: {e}", "results": []}


# 初期化処理
init_database()
```

## 4. 全体アーキテクチャ

このソフトウェアひな形は以下のアーキテクチャで構成されています：

1. **AIエージェント層**
   - LangGraphベースのReActエージェント
   - Claude 3.7 Sonnetモデルを使用
   - カスタムステートスキーマで状態管理

2. **MCPサーバー連携層**
   - 複数のMCPサーバーとの非同期通信
   - MCPツールをLangChainのStructuredToolに変換
   - 設定ファイルベースのサーバー管理

3. **RAG層**
   - Web検索（Tavily API）
   - コンテンツ抽出と保存
   - SQLiteデータベースによる永続化
   - クエリベースの検索結果取得

## 5. 使用方法

1. **環境設定**
   - `.env`ファイルにAPIキーを設定（TAVILY_API_KEYなど）
   - `mcp_config.json`でMCPサーバーの設定を行う

2. **AIエージェントの実行**
   - `create_agent()`関数でエージェントを作成
   - 作成されたエージェントグラフを使用して対話を開始

3. **RAG機能の活用**
   - エージェントはMCPツールを通じてRAG機能を利用
   - 検索→抽出→保存→検索結果取得のフローで知識を蓄積・活用

4. **カスタマイズ**
   - `prompts/system.txt`でエージェントのプロンプトをカスタマイズ
   - MCPサーバーに新しいツールを追加して機能拡張
   - データベーススキーマを拡張して保存する情報を増やす
